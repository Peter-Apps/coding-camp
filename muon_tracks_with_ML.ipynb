{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa364a11",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning using Muon Tracks in CMS  \n",
    "\n",
    "\n",
    "Expand the pseudocode below into an investigation of [muon track data](https://github.com/QuarkNet-HEP/coding-camp/tree/main/data) in the CMS detector using Machine Learning techniques. You'll need to use the documentation pages linked for guidance on how to implement the required functions.\n",
    "\n",
    "For more in-depth reading on the analysis techniques used here, see [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas and [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurélien Géron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef381959",
   "metadata": {},
   "source": [
    "## Problem  \n",
    "Can we use machine learning to improve our fitting techniques?  \n",
    "\n",
    "## Plan  \n",
    "Use machine learning regression techniques to fit model functions to CMS tracker data for a muon. This is based on the Muon Tracks activity with machine learning application.\n",
    "\n",
    "## Data  \n",
    "For muon track 1, pt = 25 GeV, particle is a muon with q= -1. muon_track_1.csv has 3 rows of header info to ignore for analysis as a pandas dataframe. Data include x,y coordinates of the muon's trajectory but not z. The tracker has a spatial resolution on the order of 0.5 cm due to physical size of the hardware elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6580100",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = pd.read_csv('https://github.com/QuarkNet-HEP/coding-camp/raw/main/data/muon_track_1.csv', skiprows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ed0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this data set look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129769f",
   "metadata": {},
   "source": [
    "## Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e737b87",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We will first use the function [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from the Scikit-Learn library to train a linear regression model. Training a model means setting its parameters so that the model best fits the training dataset using a performace measure. This is done by finding parameters in the model that minimize the Root Mean Square Error or the Mean Square Error by minimizing the *cost* function. The LinearRegression class is based on [scipy.linalg.lstsq()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) function which calculates the least squares solution to an equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model to a set of muon tracks in CMS using linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# The array must be 2D so we will reshape the data before fitting\n",
    "X = track['x'].values.reshape(-1, 1)\n",
    "Y = track['y'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d108fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model and name it lin_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98337cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model's coefficient and intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e03b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions for a new range of x-values called X_new\n",
    "X_new = np.array([.25, .5, .75, 1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad913e",
   "metadata": {},
   "source": [
    "### Linear Model Fitting using Stochastic Gradient Descent\n",
    "\n",
    "Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide variety of problems by iteratively changing parameters to minimize a cost function. Gradient Descent measures the local gradient of the error function and goes in the direction of descending gradient. The size of the step is the learning rate. \n",
    "\n",
    "We will use SGDRegressor from the sklearn library to implement machine learning techniques to fit the linear model. SGD stands for Stochastic Gradient Descent: picks a random instance in the training set at every step and computes the gradients based only on that single instance. More details about the function can be found [here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
    "\n",
    "We will perform Linear Regression using SGD for maximum 1000 epochs (max_iter=1000) or until the loss drops by less than 1e-3 during one epoch (tol=1e-3), starting with a learning rate of 0.3 (eta0=0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.3, random_state=42)\n",
    "sgd_reg.fit(X,Y.ravel())\n",
    "sgd_params = sgd_reg.coef_, sgd_reg.intercept_\n",
    "sgd_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172995ce",
   "metadata": {},
   "source": [
    "The function that determines the learning rate at each iteration is called the learning schedule. If the learning rate is reduced too quickly, you may get stuck in a local minimum or even end up frozen before reaching the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the learning rate to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the learning rate to 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe49af",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "We know that the path of a muon is not a simple straight line. We can use a linear model to fit nonlinear data by adding features to our variable X in order to use linear regression. We can now fit our data to a parabola by adding a second feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9246f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the transformed array to make a new linear model and name it poly_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5df953",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg_params = poly_reg.coef_, poly_reg.intercept_\n",
    "\n",
    "# what quantity does this print?\n",
    "poly_reg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647312df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original data and the optimized models (i.e., trendlines)\n",
    "\n",
    "def plot_linear_model(x, m, b):   # x and y are track coordinates, m & b are coefficients\n",
    "    y = m * x + b\n",
    "    return(y)\n",
    "\n",
    "plt.scatter(track['x'], track['y'], label=\"data\", color='k')\n",
    "plt.plot(track['x'], plot_linear_model(track['x'], lin_reg_params[0][0][0], lin_reg_params[1][0]), label=\"linear regression\",color='red')\n",
    "plt.plot(track['x'], plot_linear_model(track['x'], sgd_params[0], sgd_params[1]), label=\"SGD\",color='blue')\n",
    "plt.plot(track['x'], (poly_reg_params[0][0][1]*np.square(track['x']) + poly_reg_params[0][0][0]*track['x'] + poly_reg_params[1]), label=\"polynomial regg model\",color='black')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb200780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the models using a residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the models using chi-sq or residual sum of squares (RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17b1a5",
   "metadata": {},
   "source": [
    "### Regularized Models\n",
    "\n",
    "So far we have trained models setting its parameters so that the model best fits the training set. If you perform a high-degree polynomial regression, you will fit the training data much better than the Linear Regression. Fitting a 300-degree polynomial data may **overfit** the data by curving through each individual point while **underfitting** can happen when the using a function with too few degrees (like using a line to fit curved data). \n",
    "\n",
    "You can constrain the number of degrees a model uses for the fit by adjusting the hyperparameter *alpha*. ]A smaller value for alpha provides less constraint and the model will have a greater number of degrees (i.e., more curvy). A larger value of alpha makes a more-constrained model with of fewer degrees (i.e., more linear).\n",
    "\n",
    "[Skikit Learn's ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) solves a regression model where the loss function is the linear least squares function. It is a regularized version of Linear Regression and adds a regularization term to the cost function. This forces the learning algorithm to keep the weights as small as possible during training. The hyperparameter (alpha) controls how much you want to regulatize the model. \n",
    "\n",
    "Start by fitting the data to a ridge linear model and Sochastic Average Gradient descent (solver=\"sag\") with an alpha of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg_sag = Ridge(alpha=0.01, solver=\"sag\")\n",
    "ridge_reg_sag.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does increasing or decreasing alpha affect the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Ridge Model prediction to your earlier lin_reg prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e248e",
   "metadata": {},
   "source": [
    "[Least Absolute Shrinkage and Selection Operator (Lasso) Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) is another regularized version of Linear Regression. If you want the heavier math behind the difference between Ridge and Lasso, see this article on [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80460293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the Lasso regression prediction to your earlier predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What might indicate the alpha = 1 is too constrained of a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465a4b8",
   "metadata": {},
   "source": [
    "## Conclusion  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b9fa1",
   "metadata": {},
   "source": [
    "*How can you summarize the results of the different analysis methods?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54daa8d",
   "metadata": {},
   "source": [
    "---  \n",
    "## Saving Your Work  \n",
    "This is running on a Google server on a distant planet and deletes what you've done when you close this tab. To save your work for later use or analysis you have a few options:  \n",
    "- File > \"Save a copy in Drive\" will save it to you Google Drive in a folder called \"Colaboratory\". You can run it later from there.  \n",
    "- File > \"Download .ipynb\" to save to your computer (and run with Jupyter software later)  \n",
    "- File > Print to ... um ... print.  \n",
    "- To save an image of a graph or chart, right-click on it and select Save Image as ...  \n",
    "\n",
    "## Credits\n",
    "This notebook was designed by Brown University PhD student Farrah Medi Simpson and [Quarknet](https://quarknet.org/) Teaching and Learning Fellow [Adam LaMee](https://adamlamee.github.io/). The handy csv files were created from the CMS Run2011A primary datasets and converted from ROOT format by the masterful [Tom McCauley](https://github.com/tpmccauley). More can be found on the [CERN OpenData](http://opendata.cern.ch/?ln=en) site. Finally, thanks to the great folks at [Binder](https://mybinder.org/) and [Google Colaboratory](https://colab.research.google.com/notebooks/intro.ipynb) for making this notebook interactive without you needing to download it or install [Jupyter](https://jupyter.org/) on your own device.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
